# Principle 1: The Principle of Flow
As outlined in The DevOps Handbook, teams must decrease deployment time for changes while increasing service reliability and quality. this principle remains foundational for generative AI systems. However, the generative AI introduces unique challenges

[*"Our goal is to decrease the amount of time required for changes to be deployed into production and to increase the reliability and quality of those services."* ](**The DevOps Handbook: How to Create World-Class Agility, Reliability, & Security in Technology Organizations eBook : Kim bestselling author of The, Gene, Humble, Jez, Debois, Patrick, Willis, John, Forsgren, Nicole: Amazon.co.uk: Books. (n.d.). https://www.amazon.co.uk/DevOps-Handbook-World-Class-Reliability-Organizations-ebook/dp/B0F7J1WWQD?ref_=ast_author_mpb**)

## Implementing Efficient Flows

### Creating Quick Feedback Loops Through Systematic Test Suite Development
Unit testing and integration testing in software engineering can usually be defined upfront because we have an idea of the behaviour of the system we want to build, including it's edge cases. Generative AI is different. Creating tests needs an additional preliminary step of  extensively analysing traces of previous requests and responses in order to identify common failures modes. Only when these failure modes are identified and categorised can we then defining automated checks to identify those failure patterns. 

** Dangerous default** 

Teams often fall into a dangerous default pattern where one tester runs five to six queries repeatedly in a chat interface and provides pass/fail analysis. This approach fails to capture error frequency metrics or classify failure types systematically.

Teams should instead apply the scientific method iteratively. 

1. Start by categorizing failure modes as quickly as possible using approximately 100 traces. ([source](**Husain, H., & Shankar, S. (n.d.). Frequently Asked Questions (And Answers) about AI Evals – Hamel’s blog. Hamel’s Blog. https://hamel.dev/blog/posts/evals-faq/**))

2. Create simple failure reports using tools like MLflow or Jupyter notebooks with visualizations. ([methods](**Husain, H., & Shankar, S. (n.d.). Frequently Asked Questions (And Answers) about AI Evals – Hamel’s blog. Hamel’s Blog. https://hamel.dev/blog/posts/evals-faq/**))

3. Gradually calibrate automated LLM judges metrics through MLflow scorers, 

4. Translate these offline scorers to run in an online setting on production traces. Usually, this involves collecting traces into an OLAP database such as Delta tables, and then running batch jobs to evaluate a sample of these production traces using the same MLflow scorers.

5. Surface aggregated metrics in dashboards to monitor trends over time. Periodically have product managers / business SMEs review and refine test suites based on new failure modes observed in production.

### Proactively Manage Evaluation Costs

Unlike traditional unit tests, LLM-as-a-judge calls cost money. Agent traces can span significant lengths, increasing token costs per test. Teams need strategies for creating fast and affordable evaluation suites.

1. Reduce tokens sent to LLMs. 

Rather than sending entire traces, implement preprocessing functions that extract relevant tool spans for testing functions. Experiment with reducing output tokens since output generation creates the primary bottleneck. Test more efficient text formatting approaches and replace JSON formatting with XML or free text alternatives. Experiment with reducing output tokens since output generation creates the primary bottleneck. Test more efficient text formatting approaches and replace JSON formatting with XML or free text alternatives.

- [link1](_Optimizing LLM prompts for low latency | Building with AI_. (n.d.). incident.io. https://incident.io/building-with-ai/optimizing-llm-prompts#case-study-planning-grafana-dashboards-for-an-incident) 
- [link2](Hamel Husain. (2025, August 15). _From Noob to Automated Evals In A Week (as a PM) w/Teresa Torres_ [Video]. YouTube. https://www.youtube.com/watch?v=N-qAOv_PNPc)
- [link3](_LLM Inference Performance Engineering: Best Practices | DataBricks blog_. (n.d.). Databricks. https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)

2. Use non-LLM-based checks when possible.
Deterministic tests that compare received and expected outputs in a rule-based way is both faster and cheaper than using an LLM. For example, use keyword matching to identify whether a customer support ticket issue classifier returns one of a predefined list of categories. This approach eliminates inference costs entirely for certain test categories.

3. Separate evaluation datasets into distinct classes of test suites.
CI tests can be run quickly by limiting the number of input examples to a small number, for example approximately 100 examples. The main goal is to identify regressions before production deployment. 

Pre-production test suites can run on a larger sample of live traces through an offline batch job. The goal here is different to that of CI testing. Here, we are less time-constrained, and we want a large enough sample to allow us to identify new, uncategorized errors for triage.

Tuning Automated LLM Judges for Reliability

LLM judges require careful calibration to ensure consistent, reliable evaluations across different scenarios and edge cases.

### Enabling Easy Rollbacks During Failures
Teams need robust rollback capabilities when agent or tool versions fail. These can be done through different mechanisms
- MLflow model versions in the MLflow model registry enable quick version management. 
- API endpoints can revert to previous model versions instantly. 
- Prompt registries maintain version control for prompt iterations. 
- Central repositories should store tested tools with proper versioning to ensure reliable rollback capabilities.

#### Example: ML Model Lifecycle Workflows in Unity Catalog

```

#### Before Deployment: Add Model Metadata
- Track data lineage using `mlflow.log_input` to link model to training datasets
- Add descriptions, tags, and version-specific information through UI or API

---

##### Create Champion-Challenger Model Setup

###### Step 1: Create Challenger Model
- Register new model version using same process as above
- Version numbers are automatically incremented (e.g., version 1, 2, 3...)

###### Step 2: Set Model Aliases
- Assign "Champion" alias to current production model and "Challenger" alias to new version:
```python
from mlflow import MlflowClient
client = MlflowClient()

###### Set Champion (current production)
client.set_registered_model_alias("prod.ml_team.model_name", "Champion", 1)

###### Set Challenger (new candidate)
client.set_registered_model_alias("prod.ml_team.model_name", "Challenger", 2)
```

###### Step 3: Deploy for A/B Testing
- Reference models by alias in inference workloads:
```python
# Load Champion model
champion_model = mlflow.pyfunc.load_model("models:/prod.ml_team.model_name@Champion")

# Load Challenger model
challenger_model = mlflow.pyfunc.load_model("models:/prod.ml_team.model_name@Challenger")
```

###### Step 4: Promote Challenger to Champion
- After validation, reassign aliases:
```python
# Promote Challenger to Champion
client.set_registered_model_alias("prod.ml_team.model_name", "Champion", 2)

# Optionally retire old Champion
client.delete_registered_model_alias("prod.ml_team.model_name", "Challenger")
```

---

##### Model Rollback Process

###### Step 1: Identify Rollback Target
- List available model versions:
```python
client = MlflowClient()
versions = client.search_model_versions("name='prod.ml_team.model_name'")
```

###### Step 2: Quick Rollback Using Aliases
- Reassign "Champion" alias to previous stable version:
```python
# Rollback Champion alias to version 1
client.set_registered_model_alias("prod.ml_team.model_name", "Champion", 1)
```

###### Step 3: Update Deployment Systems
- Batch inference workloads automatically pick up the rollback on next execution when using aliases
- For model serving endpoints, update via REST API:
```python
champion_version = client.get_model_version_by_alias("prod.ml_team.model_name", "Champion")
# Update serving endpoint with rollback version
```

###### Step 4: Verify Rollback
- Test rolled-back model in staging environment
- Monitor performance metrics to confirm successful rollback
- Document rollback reason and actions taken

This workflow ensures safe model deployment with quick rollback capabilities while maintaining full governance and traceability.

