# Principle 1 The principle of flow

Devops principles still apply (as outlined in The DevOps Handbook) 

[*"Our goal is to decrease the amount of time required for changes to be deployed into production and to increase the reliability and quality of those services."* ](**The DevOps Handbook: How to Create World-Class Agility, Reliability, & Security in Technology Organizations eBook : Kim bestselling author of The, Gene, Humble, Jez, Debois, Patrick, Willis, John, Forsgren, Nicole: Amazon.co.uk: Books. (n.d.). https://www.amazon.co.uk/DevOps-Handbook-World-Class-Reliability-Organizations-ebook/dp/B0F7J1WWQD?ref_=ast_author_mpb**)

## Implementing efficient flows
Industry best practices already exist and have been covered elsewhere. These practice include:

	- make small commits
	- limit the number of handoffs needed for deployment

In this section we highlight the practices that need to be rethought for Generative AI, specifically the practices of creating fast, reliable and automated tests. 

### Creating quick feedback loops through fast test suites
		
#### Differences in test suites between software engineering and generative AI

**Significant component of data analysis of failure modes** 
				
- Dangerous default: one tester runs 5-6 queries in a chat interface again and again and provides pass/fail analysis
				
	- no metrics on how often errors occur
				
	- no classification of metrics
				
- Recommendation: apply the scientific method iteratively:
				
	- categorise failure modes as soon as possible using ~100 traces ([methods](**Husain, H., & Shankar, S. (n.d.). Frequently Asked Questions (And Answers) about AI Evals – Hamel’s blog. Hamel’s Blog. https://hamel.dev/blog/posts/evals-faq/**))
				
	- start with a simple failure report eg. with MLflow / Jupyter notebook with visualisations ([methods](**Husain, H., & Shankar, S. (n.d.). Frequently Asked Questions (And Answers) about AI Evals – Hamel’s blog. Hamel’s Blog. https://hamel.dev/blog/posts/evals-faq/**))
				
	- start to calibrate automated metrics (MLflow scorers)
				
	- apply MLflow scorers to online setting

**Tests can become expensive!** 
				- LLM-as-a-judge calls are not free like traditional unit tests 
				- Traces from agents can be long, which increases token cost per test. 
				- Strategies for creating fast (and cheap!) evaluation suites
					- sources: [link1](_Optimizing LLM prompts for low latency | Building with AI_. (n.d.). incident.io. https://incident.io/building-with-ai/optimizing-llm-prompts#case-study-planning-grafana-dashboards-for-an-incident) [link2](Hamel Husain. (2025, August 15). _From Noob to Automated Evals In A Week (as a PM) w/Teresa Torres_ [Video]. YouTube. https://www.youtube.com/watch?v=N-qAOv_PNPc)[link3](_LLM Inference Performance Engineering: Best Practices | DataBricks blog_. (n.d.). Databricks. https://www.databricks.com/blog/llm-inference-performance-engineering-best-practices)
					- 1. reduce number of tokens sent to LLM
						- rather than sending the entire trace, have a pre-processing function that extracts the relevant tool span to pass to a testing function
						- experiment with different reducing number of output tokens because output token generation is the bottleneck when returning a response 
						- experiment with more efficient text formatting 
						- remove JSON formatting and using XML / free text
					- 2. If possible, use strategies such as keyword matching instead of LLM judges 
					- 3. Separate evaluation dataset into different tests suites
						- CI tests for identifying regressions before production ~100 examples
						- production pipeline runs tests on live traces and identifies new and uncategorised errors to be triaged


**Need to tune automated LLM judges for reliability**
				Reference Shreya? 
				Talk to Eric Peter 
		
	- Easily rollback agent and tool versions during failure
		- MLflow model versions in MLflow model registry (add screenshot here)
		- rollback API endpoints to previous model version (add screenshot)
		- Prompt registry (screenshot)
		- tool versioning? tested tools available in central repository
    