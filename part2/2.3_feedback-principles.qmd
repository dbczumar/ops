# Principle 2 The principle of feedback

## Background context

For experienced software engineers, collecting comprehensive telemetry is not new. An engineer developing a web server would monitor everything from web page loading times, database query latencies, disk space and networking latencies. These metrics are usually operational in nature and easily quantifiable. 

Traditional machine learning practitioners meanwhile, are more narrowly focused. They focus on metrics related to model performance such as precision, recall and root mean squared errors. 

## Current State of AI System Monitoring

Given that Generative AI applications blend both software development and machine learning, Gen AI developers need to be aware of approaches to getting feedback from the systems they build. In additional, they also need to be ware of how to handle additional forms of feedback that arise because of the natural language output from Large Language models. These feedback types include: 

- subject-matter expert feedback
- end-user feedback
- LLM judge feedback
- programmatic code checks

![feedback-types](feedback-principles_files/mlflow-feedback-diagram.png)

These additional sources of feedback are collected at different points of the application development lifecycle. 

- **pre-production**: a subject-matter expert checks for a responses veracity
- **production / beta-testing**: after a response is returned, an end-user provides feedback through comments or emojis about whether they got the information they needed or whether their task was completed to their satisfaction. Even while an agent is generating a response, we may also trigger asynchronouns evaluation checks on the agent to check for things such as not including sensitive information in a response. By running checks during response generation instead of after, we can correct the agent in real-time if needed. 
- **pre and post-production** LLM-judges running on a sample of traces allows us to scale evaluation to more traces than what a human might handle, while also acting as an automated fail-safe during the CI/CD process. 

## Worked Example: Customer Email Generator

Consider an AI application that helps account managers with customer communications. The system generates different types of emails including meeting follow-ups and cold outreach messages, using context from previous customer interactions. In order to assess our application's quality, several sources of feedback are needed:

1. Account managers (SMEs) verify if the email accurately reflects meeting discussions
2. Account managers (end-users) indicate whether the generated email was helpful and saved them communication time
3. LLM judges evaluate email relevance, tone and professionalism
4. Automated rule-based checks ensure emails follow company guidelines

Each feedback source helps improve the system's ability to generate appropriate, contextual customer communications.

#### Feedback is iterative
Collecting feedback by itself is not enough to ensure a quality application. In contrast to more traditional system metrics, such as webpage response times, that are informative as is, feedback for generation AI applications usually needs to be further categorised and analysed before it can be incorporated into application improvements. Generative AI engineers will typically need to go through a process of analysing and categorizing traces generated by the application. This is important for calibrating LLM judges and developing a taxonomy of errors that may occur in production

For example, when we start application development, we may set up a generic "Relevance" LLM assessor that:
- provides a binary Pass / Fail assessment on whether the email generated was relevant given the provided context
- provides a rational for its decision. 

![LLM relevance judge output](./feedback-principles_files/relevance-judge-output.png)

However, an actual account manager SME might disagree with this feedback and provide a correction through the labelling UI

![SME feedback](./feedback-principles_files/relevance-sme-feedback.png)

Given this feedback, we might decide that the original Relevance metric is too general. Instead, we should break down the metric further to assess:

- did the solutions in the email adequately address the customer's stated concerns and pain points
- if there was a product upsell, was it phrased in a tasteful and relevant way

Update metric description 

#### Analyse feedback manually first, then automate the process

Also cite Hamel Husain and Shreya on the errors of LLM evaluation types (three gulfs)

Align judges (add UI screenshots here )

Conclusion: 

This iterative feedback process highlights the importance of the art of translating business requirements to technical specifications. There has been some debate about whether this translation layer should be done by product managers or domain experts, or even engineers who now need to become more "product-minded". Regardless of how responsibilities are allocated, the key is creating a collaborative environment where technical and business teams can jointly discover what's possible with generative AI while maintaining high quality standards.

## Feedback flow

The step-by-step workflow of collecting feedback for a generative AI application is not very different from a software developer identifying issues in development and production, then fixing bugs and writing tests to guard against future regressions

Once again, however, there are some differences to keep in mind: 

- evaluation criteria harder to define upfront in the same way that specifying edge cases for unit testing can be done upfront
- detailed trace analysis is needed to identify quality issues. In this way, tests are "discovered" rather than "specified"
- the overlap between generative AI and traditional Data Science becomes more obvious during this trace analysis stage. Developers have to use data science-centric tools such as charts, graphs, segmentation and summary statistics to identify quality issues
- additionally, input from SME reviewers is needed to properly triage errors and develop evaluation criteria. This tasks, again, is similar to how a data science to get feedback from domain experts on the validity of an error analysis

![feedback-flow](./feedback-principles_files/feedback-flow-diagram.png)

This diagram shows a feedback collection workflow for developing a generative AI application with both offline and online components.

**Pre-Production**:

SMEs manually review and label data through sessions
Developers analyze traces to identify and fix quality issues
An evaluation dataset is built containing inputs, outputs, and metrics
Automated evaluation suites test the application

**Production**:

Application deploys with tracing enabled (first to beta, then full production)
Real-time monitoring dashboard tracks performance metrics
Users provide feedback directly in the app via feedback collection API
Production traces are continuously evaluated

**Continuous Improvement**:
The system creates a feedback loop where evaluation results inform further application iterations. Offline evaluation uses the dataset during development, while online feedback comes from production users and automated monitoring. Selected traces and human reviews are added back to the evaluation dataset to improve future iterations.
The workflow ensures quality through human oversight in development and automated monitoring plus user feedback in production.

source: https://www.sh-reya.com/blog/ai-engineering-flywheel/

## Best practices for collecting feedback

![feedback-best-practices](./feedback-principles_files/mlflow-feedback-best-practices.png)

https://mlflow.org/docs/latest/genai/tracing/collect-user-feedback/#mlflow-feedback-collection-best-practices
