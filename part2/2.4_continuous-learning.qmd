# Principle 3 The technical practices of learning

Generative AI is a field that changes fast. What was hard to accomplish yesterday might become easy tomorrow. For instance, as of 2025, post-training methods have greatly improved a model's ability to handle instructions and tool use, which has enabled agentic applications that were not feasible a year ago. This constant change makes it imperative that both business and technical stakeholders continuously update their mental models of what products are possible for them to build. 

To accomplish this, organizations can enable institution-wide continual learning by encoding hard-won knowledge into assets different teams can reliably reuse to increase their changes of success. Examples of assets include:

- standardised frameworks
- reusable reference architectures 
- industry / enterprise-specific agentic design patterns

## Context

- An organisation does not want to build only one successful generative AI application. Ideally, they are able to repeatedly produce high quality applications that can accelerate business outcomes, and to do this across multiple departments.
- We've talked to large organisations who estimate up to hundreds of potential use cases 
- These customers typically come to us and ask not only how to ensure the quality of a single POC or MVP, but how to build reliable best practices that allow an organisation to scale how the organisation applies generative AI to improve business practices. 
- Scaling generative AI best practices across an organisation can be achieved in a few ways:
    - institutionalizing technical best practices through standardised frameworks, reference architectures and best practices documentation
    - investing in continuously refining how business and technical stakeholders work together to uncover how to use generative AI to accomplish their goals

## Standardised frameworks
Standarised development frameworks allow us to achieve consistent, reliable, and efficient delivery at scale. An efficient framework aims to make software development and deployment feel like a well-oiled assembly line rather than a series of one-off custom projects, ultimately enabling faster time-to-market with higher quality and lower risk.

An example of a standardised framework that can be used for generative AI applications is [Databricks Asset Bundles (DAB)](https://github.com/databricks/mlops-stacks)

A Databricks Asset Bundle is a deployment unit that packages together all the related resources needed for a complete data/ML workflow or application.
Specifically, it's a:
- Resource Collection: Contains notebooks, jobs, pipelines, models, libraries, and configuration files that work together as a cohesive unit.
- Infrastructure-as-Code Package: YAML configuration files specify not just the code paths, but also the compute resources, permissions, schedules, and environment settings needed to run an application.
- Deployment Artifact: Can be versioned, promoted across environments (dev → staging → prod), and deployed atomically as a single unit.
- Dependency Container: Handles all the interconnections between different Databricks resources - how jobs trigger each other, which notebooks depend on which libraries, etc.

### Development lifecycle using Databricks Asset Bundles:
1. Create a skeleton project folder structure by initializing a default bundle template or a custom bundle template
2. Update the bundle configuration files by specifying settings such as workspace details, artifact names, file locations, job details, and pipeline details. These details can be customised according to different development, staging, and production deployment targets
3. Add source code to relevant folders in the project folders
4. Verify that definitions in the bundle configuration files are valid by running the Databricks CLI tool `databricks bundle validate`
5. Deploy the jobs, agents, vector databases, and other artefacts that make up the generative AI application by running `databricks bundle run` 

### Benefits of standardised frameworks
A framework like Databricks Asset Bundles provides a templatized end-to-end definition of a deployable project. This approach provides several benefits: 
- The templatized structure enabled standardisation across development teams rather than each team developed their own standards. 
- Because project and resource definitions are captured as YAML files, they can be managed by version control systems
- Projects can be deployed using external CI/CD tools such as Github Actions and Azure DevOps

## Reference architectures

We covered reference architectures in detail in Part 1. We mention them again here in the context of organizational learning because they are valuable tools for scaling architectural knowledge and best practices across an organization. 

Reference architectures provide teams with a proven blueprints containing established design patterns and best practices for building agentic applications. For example, a regulated industry might recommend that all requests and responses to an agent endpoint need to pass through a guardrail step that filters out Personal Identifiable Information (PII) from LLM outputs. 

Reference architectures create a shared understanding across teams about how systems should be built and why. 

They also capture 70-80% of the architectural decisions upfront, leaving teams to focus on the 20-30% that's specific to their unique requirements.

## Industry / enterprise-specific agentic design patterns

Reference architectures are useful for outlining how different components of a technology stack integrate with each other. But it's also useful to have shared blueprints that drill down specifically into how agents themselves should be designed. 

The reason for this is that, although the consensus is that generative AI has the potential to automate and enhance workflows across industries, the details of how we should tailor agent design to the particularities of an industry remains vague.Teams often have to learn as they develop. This hard-won knowledge should be shared as design patterns that can be adopted across an enterprise so teams do not have to reinvent the wheel. 

Example 1: Insurance claims processing 

There are a few domain-specific aspects of insurance claims processing that influence how agents need to be built. 

- **Claims processing is an event-driven process:** When a claim is first received, the event may trigger a document review process. If review process identifies missing documents, then another event is triggered: a webhook is called that sends a customer a follow-up email asking for the missing documents. At this point, the processing workflow needs to be paused until the next event (an email reply from the customer) comes in. 

- **Claims workflows need the ability to be paused and resumed** The event-driven workflow outlined above implies that, at crucial juncture, the agents graph needs to be paused and then resumed at a later point in time. 

In terms of agent design, engineers need to plan for a state store that can create a checkpoint with details on the paused step. This checkpoint will be referenced when the workflow is resumed. Additionally, the agent subgraph encoding the agentic workflow will also need to be designed such that it can resume processing at specific points in the subgraph rather than needing to be re-run from the beginning. 

- **Agent state depends on both local and global events** To add another layer of complexity, here may be parallel events (such as registering claims event with a third-party regulatory authority) that needs to be registered and taken into account when the workflow is resumed in order for the agent to have the most up-to-date context. 

These features - an event-driven architecture, pause-and-resume capability, and a state store is something that will likely be shared across different claims departments. Developers building agentic solutions for each department would benefit from shared design elements such as a common schema outline for the state store, or a common JSON payload structure used to trigger a resume event. 

Example 2: Financial services compliance 

Within the financial services industry, regulatory bodies usually periodically release new regulations and reporting requirements that compliance departments need to follow. Monitoring for new regulatory requirements and desiging appropriate compliance checks used to be a manual process done by humans. However, organizations are increasingly interested in having this processe automated by an agent. 

As with the example of insurance claims processing, there are shared elements of agent design that should be documented and reused by agents implemented for various compliance and audit-related use cases. 

Agent handovers
Regulatory assessor agent receives new regulation released by a government regulatory authority - outputs set of recommendations, assessments on potential gaps in existing compliance processes
Regulatory checks proposed by Agent
Handover to another agent to run and validate these checks conform to 

Each step of the process might require multiple tool-calls and reasoning steps, making these use case unsuitable to be executed a pre-defined chain of LLM calls. 
Agent handover, what information is included for each handover, is crucial for 

Teams should design accordingly.
