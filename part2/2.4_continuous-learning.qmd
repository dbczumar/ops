# Principle 3 The technical practices of learning

Generative AI is a field that is rapidly changing. What was hard to accomplish yesterday might become easy tomorrow. For instance, as of 2025, post-training methods have greatly improved a model's ability to handle instructions and tool use, which has enabled agentic applications that were not feasible a year ago. This constant change makes it imperative that both business and technical stakeholders continuously update their mental models of what products are possible for them to build. 

To accomplish this, organizations can enable institution-wide continual learning by encoding hard-won knowledge into reusable assets such as: 

- standardised frameworks
- reusable reference architectures 
- communities of practice for both business and technical teams

## Context

- An organisation does not want to build only one successful generative AI application. Ideally, they are able to repeatedly produce high quality applications that can accelerate business outcomes, and to do this across multiple departments.
- We've talked to large organisations who estimate up to hundreds of potential use cases 
- These customers typically come to us and ask not only how to ensure the quality of a single POC or MVP, but how to build reliable best practices that allow an organisation to scale how the organisation applies generative AI to improve business practices. 
- Scaling generative AI best practices across an organisation can be achieved in a few ways:
    - institutionalizing technical best practices through standardised frameworks, reference architectures and best practices documentation
    - investing in continuously refining how business and technical stakeholders work together to uncover how to use generative AI to accomplish their goals

## Standardised frameworks
Standarised frameworks allow us to achieve consistent, reliable, and efficient delivery at scale. An efficient framework aims to make software development and deployment feel like a well-oiled assembly line rather than a series of one-off custom projects, ultimately enabling faster time-to-market with higher quality and lower risk.

An example of a standardised framework that can be used for generative AI applications is Databricks Asset Bundles (DAB)

A Databricks Asset Bundle is a deployment unit that packages together all the related resources needed for a complete data/ML workflow or application.
Specifically, it's a:
- Resource Collection: Contains notebooks, jobs, pipelines, models, libraries, and configuration files that work together as a cohesive unit.
- Infrastructure-as-Code Package: YAML configuration files specify not just the code paths, but also the compute resources, permissions, schedules, and environment settings needed to run an application.
- Deployment Artifact: Can be versioned, promoted across environments (dev → staging → prod), and deployed atomically as a single unit.
- Dependency Container: Handles all the interconnections between different Databricks resources - how jobs trigger each other, which notebooks depend on which libraries, etc.

### Development lifecycle using Databricks Asset Bundles:
1. Create a skeleton project folder structure by initializing a default bundle template or a custom bundle template
2. Update the bundle configuration files by specifying settings such as workspace details, artifact names, file locations, job details, and pipeline details. These details can be customised according to different development, staging, and production deployment targets
3. Add source code to relevant folders in the project folders
4. Verify that definitions in the bundle configuration files are valid by running the Databricks CLI tool `databricks bundle validate`
5. Deploy the jobs, agents, vector databases, and other artefacts that make up the generative AI application by running `databricks bundle run` 

### Benefits of standardised frameworks
A framework like Databricks Asset Bundles provides a templatized end-to-end definition of a deployable project. This approach provides several benefits: 
- The templatized structure enabled standardisation across development teams rather than each team developed their own standards. 
- Because project and resource definitions are captured as YAML files, they can be managed by version control systems
- Projects can be deployed using external CI/CD tools such as Github Actions and Azure DevOps

## Reference architectures

(customer story?)

Reference architectures provide teams with a proven blueprints containing established design patterns and best practices for building agentic applications. For example, a regulated industry might establish that all requests and responses need to pass through a guardrail step that filters messages for PII information. 

Creates shared understanding across teams about how systems should be built and why.

Provides 70-80% of the architectural decisions upfront, leaving teams to focus on the 20-30% that's specific to their unique requirements.

## Technical best practices

## Collaborative Metric Development and LLM-as-a-Judge Calibration

One of the most critical challenges in AgentOps is establishing effective evaluation criteria. Unlike traditional software and machine learning where success metrics are often clear-cut, evaluating agentic AI requires synthesizing knowledge scattered across different departments—from technical teams who understand system capabilities, to business leaders who own outcomes, to Subject Matter Experts (SMEs) who define quality in domain-specific contexts.

Without this unified view, teams struggle with ambiguous success criteria and waste effort measuring against disparate, unwritten standards. It isn't possible to calibrate an automated judge if the success metric itself is not clear.

### Building Shared Understanding Through Cross-Functional Collaboration

When a technical lead, a domain expert, and a product manager review the same customer service response, they often disagree on what makes it "good" or "bad"—and that disagreement is exactly what you need to surface and resolve.

Start by gathering concrete examples of your agent's outputs, ideally covering the range of scenarios you expect in production. Bring together representatives from each stakeholder group and work through these examples systematically. The technical team might focus on whether the agent used tools correctly, while the domain expert cares about accuracy and the product manager evaluates user experience.

These sessions reveal the gaps between what different groups consider success. A response that's technically correct might still frustrate users if it's too verbose. An answer that delights users might concern compliance teams if it makes claims beyond the agent's knowledge base.

The goal isn't to eliminate these different perspectives but to make them explicit and find ways to measure all the dimensions that matter. This usually means developing multiple evaluation criteria rather than trying to find a single "quality score."

Once you've established shared criteria through human review, you can scale the evaluation using LLM judges trained on your team's collective understanding. These judges won't replace human oversight entirely, but they can handle the bulk of routine evaluation while flagging edge cases for human review.

At Databricks, we've developed a research-backed workshop to help teams navigate this process more efficiently. Our GenAI specialists guide cross-functional teams through the discovery process using custom applications designed to streamline the collaborative review and automated judge calibration.

This workshop is designed to solve that problem by bringing technical, business, and SME stakeholders together. By collectively reviewing concrete examples of agent responses—a process we call the Discovery Phase—your group will create a single, shared definition of what constitutes a "good" versus a "bad" response. This crucial collaboration allows us to define exactly WHAT we want to measure and translate your team's collective qualitative expertise into a quantitative framework for success. The ultimate outcome is not just an agreed-upon rubric, but a calibrated LLM-as-a-Judge that automatically scales this shared knowledge to reliably evaluate all future agent performance, saving countless hours and ensuring your AI is measured against metrics that truly matter to your business.

## Business stakeholder education

Generative AI is not only a paradigm shift for how developers build applications, but how business users interact and use applications as well. 

First MVPs are hard because both developers and business users go through a journey learning about what is and is not possible. 

For developers, they have to fill in their knowledge gaps. Software engineers might be new to the idea of data analysis and developing LLM judges. Machine Learning practitioners can be surprised by the amount of ETL and UI code they have to produce. Business users also may start with high expectations that may have to be calibrated 

Example 