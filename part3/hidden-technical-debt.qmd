# Hidden Technical Debt in Agent Systems

Companies like OpenAI and Anthropic release new model capabilities every few months. These models enable developers to build agentic systems with more complex tool-calling capabilities that were not possible before. Engineering teams, accustomed to longer development cycles, struggle to integrate resilience practices into these accelerated timelines.

**Technical debt** accumulates when we ship quick fixes that compromise long-term reliability, safety, and maintainability. 
What follows is a curated set of the most common, high-impact hidden debts in agentic systems and how cross-functional teams can control them.

## The Debts We Already Knew

Classical ML taught us three recurring debts (Sculley et al., [Machine Learning: The High-Interest Credit Card of Technical Debt](https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/43146.pdf)):

- **Data debt** that come from data dependencies that are poorly documented, unaccounted for or that change silently.
- **System level debt** that can come from extensive glue code, pipeline “jungles” and “dead” hardcoded paths.
- **External changes** that change fixed thresholds (such as the precision-recall threshold) or remove previously important correlations.

Agent systems inherit all of these, and add new, distinctly *agentic* forms of debt.

## Tool Sprawl

As more tools are added (APIs, SQL functions, workflow triggers), catalogs get messy: duplicates, near-duplicates, and unclear ownership. 
The agent must first pick the right tool, then map user intent to the right parameters, both are frequent failure points when tools overlap.

**How to identify**

- Similar tools (weekly vs monthly sales) both appear in traces
- PRs re-implement existing adapters. There is an unclear “source of truth”
- Wrong tool selected or parameter mismatches in error logs
- Teams can’t list available tools or owners

**Impact**

- Accuracy drops from mis-routing. Higher latency and cost from retries
- Harder evaluation and debugging. Fragile behavior across environments
- Security and governance gaps when tools bypass review

**How to control**

- Maintain a tool registry (owner, purpose, version, examples, scope). As an option, use MCP for typed schemas, discovery, and central policy to deduplicate adapters and reduce wrong-tool calls
- Consolidate or deprecate overlaps. Prefer fewer, clearer tools
- Add tool-choice and parameterization evals, plus schema validation at call time
- Keep chains simple. Introduce multi-agent orchestration only when metrics justify it

## Prompt Stuffing

Prompts grow into “god prompts” as teams append rules for new edge cases. Length alone isn’t the problem: contradictions, staleness, and order-dependence are making behavior brittle and slow to iterate.

**How to identify**

- Prompts span pages. Prompts include comments warn “don’t edit above”
- Changes in one section break behavior in another
- Rising incidents traced to conflicting or outdated instructions

**Impact**

- Inconsistent outputs. Harder A/B testing and root cause analysis (RCA)
- Slower iteration. Hidden coupling between rules
- Drift from policy and source of truth

**How to control**

- Split into smaller, role/task prompts 
- Retrieve long policies/FAQs
- Version prompts 
- Test with golden sets and A/B or offline evals
- Adopt prompt templates/patterns
- Document intent and scope
- Remove stale instructions 
- Enforce review for prompt changes

## Opaque Pipelines

Without step-level tracing, multi-stage flows hide where failures occur (retrieval, rewriting, tool calls, formatting) resulting in teams guessing at fixes, and a growing time to restore service.

**How to identify**

- “Model issue” appears as a default incident label
- Disagreement on which step failed 
- Fixes feel like guesswork
- Traces (if any) show only final inputs/outputs

**Impact**

- Slow debugging
- Repeated regressions, wasted spend
- Hard to reproduce issues across dev/staging/prod
- Poor prioritization resulting in teams fixing the wrong step

**How to control**

- Trace each step (inputs/outputs, latency, errors) under a root trace
- Tag versions (index/embedding/prompt/tool)
- Route tool invocations through MCP so each call emits consistent, structured metadata (tool name/version, args, result) that slots cleanly into your traces
- Add a failure taxonomy
- Start with top incident spans, then expand coverage
- Use trace replay for regression checks before promotion

## Model drift

Even without code changes, behavior shifts as input distributions, prompts, tools, or the underlying model evolve. Quality slowly decays, routes change, and budgets are breached without an obvious “event.”

**How to identify**

- Scheduled eval scores trend down on stable test sets
- Change-point signals on tone/groundedness or tool-choice accuracy
- Rising cost/latency at the same traffic mix
- A/B or shadow runs show divergence vs the pinned baseline

**Impact**

- Degraded task success and safety regressions
- Tool mis-selection and inconsistent routing
- Breached SLOs and unplanned spend

**How to control**

- Pin versions of models/prompts/tools; record them in traces
- Canary/shadow new versions; replay historical traces before promotion
- Set drift alerts on quality, cost, and latency. Auto-roll back on breach
- Refresh prompts or retrain adapters on measured drift, not hunches

## Data quality issues

When the knowledge base, indexes, or tool schemas drift from what the agent expects, small mismatches cascade: stale facts, broken chunking/metadata, silently renamed fields, or out-of-date embeddings lead the agent to retrieve the wrong context or parse tool responses incorrectly.

**How to identify**

- Traces show relevant doc missing or low retrieval scores despite known coverage
- Spike in “no answer” or hallucination tags after a data refresh
- Mismatched index/embedding versions across environments
- Tool responses change shape; parameter validation errors rise

**Impact**

- Lower accuracy and trust, rising re-tries and cost, longer latency from repeated retrieval
- Hard-to-reproduce bugs across dev/staging/prod due to hidden version skew

**How to control**

- Data contracts and validation on ingest; freshness SLAs and dashboards
- Version and tag indices/embeddings in traces; schedule re-embed/reindex
- Metadata hygiene: right-size chunks, consistent IDs, required filters
- Change management: diff samples pre/post refresh; canary the new index

## Infrastructure complexity

Infrastructure complexity is the debt that occurs when a simple agent stack grows into a mesh of components orchestator, retriever and index, tool gateway, queues for workers, safety filters, observability and more roles and configs. Each request now takes more hops, ownership blurs, configs drift, and small changes require touching multiple services, increasing fragility and the time to restore service.

Example: "returns agent" before, during MVP stage` vs after when several capabilities have been added and the infrastructure complexity has increased. 

#### Before (MVP) {-}
```{mermaid}
flowchart LR
    %% Infrastructure complexity — "returns agent" before
    User --> AppServer
    AppServer --> BillingAPI
    BillingAPI --> Response
```

#### After (scaled) {-}
```{mermaid}
flowchart TD
     %% Infrastructure complexity — "returns agent" after
    User --> APIGateway
    APIGateway --> Orchestrator
    Orchestrator --> RetrieverIndex
    Orchestrator --> ToolGateway
    ToolGateway --> FraudAPI
    ToolGateway --> ImageClassifier
    ToolGateway --> WarehouseAPI
    ToolGateway --> PaymentsAPI
    Orchestrator --> WorkQueue
    WorkQueue --> HumanReview
    Orchestrator --> Communicator
    Communicator --> Response

    %% Failure chain from a tiny upstream change
    FraudAPI --> SchemaChange
    SchemaChange --> AdapterRetries
    AdapterRetries --> QueueBacklog
    QueueBacklog --> Reroutes

```

**How to identify**

- Traces show excessive hops/spans with frequent circular routes or timeouts
- On-call toil: many handoffs to fix a single incident
- Dependency maps are out of date. Unclear ownership for tools/agents
- Rollbacks are slow or manual. Environment drift is common

**Impact**

- Higher latency and cost, more incident volume, slower recovery
- Hard onboarding and brittle deployments

**How to control**

- Simplify topology, prefer fewer roles/agents that clearly earn their keep
- Maintain an agent/tool registry with owners and deprecation paths
- Standardize tracing/metrics/logging, enforce SLOs and budgets at boundaries
- Infrastructure as code with staged deploys and one-click rollback
- Periodic “debt sprints” to remove dead code, unused tools, and redundant queues

## Inadequate Human Feedback Loops

It’s easy to ship something that “looks good” in demos but misses real-world nuance. Without a feedback loop, quality plateaus and blind spots persist.

**How to identify**

- Users report “not what I meant” despite high offline scores
- Feedback lives in Slack/email, not a dataset
- Little correlation between changes and measured improvement

**Impact**

- Stagnant quality 
- Low trust from domain users
- Missed product fit
- Repeated mistakes
- Difficulty prioritizing fixes

**How to control**

- Define success metrics early (task success, groundedness, latency, cost)
- Provide a feedback UI. Collect structured labels and comments
- Schedule automated evals. Feed errors back into tests/prompts/tools
- Regular reviews with domain experts using clear rubrics

## Building Without Regular Stakeholder Check-ins

Scope and expectations drift when sponsors, users, and adjacent teams aren’t engaged. Teams over-promise on LLM capabilities or build the wrong thing well.

**How to identify**

- Late “Can it also…?” requests; surprise compliance blockers
- Conflicting priorities between teams; unclear “done” criteria
- MVPs that don’t solve the top user problems

**Impact**

- Rework and churn; credibility hits
- Slip in delivery timelines; misallocated budget
- Shadow processes to bypass missing approvals

**How to control**

- Set high-touch cadences with users/sponsors; publish decisions
- Keep a living in/out of scope list per release
- Educate on capability limits, reset expectations with measured results
- Align on acceptance criteria tied to your eval metrics

## Safety & Security

Agents interact with untrusted inputs (users, webpages, retrieved docs) and powerful tools (APIs, databases). Without explicit safeguards, they can be steered into harmful content, data leaks, or unintended actions, often via subtle prompt-injection or poisoned context rather than “obvious hacks.”

**How to identify**

- Traces show instruction-following from retrieved text (e.g., “ignore previous rules”) preceding tool calls
- PII in logs/outputs (user names, emails, phone numbers, addresses, IDs show up in traces or chat transcripts)
- Secrets in logs/outputs (API keys, auth tokens, DB passwords or connection strings appear in error dumps or tool responses)
- Redaction misses, masking fails on edge cases or only partial fields are redacted
- Unusual data egress, calls to unexpected domains or regions, large payloads, or spikes in outbound traffic unrelated to user actions
- Spikes in high-risk actions (refunds, deletes) or calls to new/unknown endpoints
- Safety classifiers flag toxicity/self-harm/abuse in inputs/outputs
- Anomalous routing or role jumps (e.g., communicator invoking billing tools)
- Post-mortems find over-broad permissions or missing approvals on sensitive steps

**Impact**

- Data exposure and compliance breaches 
- Customer trust damage
- Irreversible side-effects (fraudulent credits, data deletion)
- Incident volume and time to restore service increase due to hard-to-reproduce injections
- “Shadow fixes” (ad-hoc prompt edits) that regress later

**How to control**

- Policy at I/O boundaries: validate and sanitize inputs, content safety and PII detection/redaction on inputs and outputs before/after the model
- Least privilege: per-role allowlists for tools/actions, scoped credentials, no default write access, environment/network egress allowlists
- High-risk gates: approvals or human-in-the-loop for sensitive actions, confidence/threshold checks, time/token/hop budgets to curb looping attacks
- Context hygiene: provenance for retrieved docs, snippet hashing, and allow only trusted sources, detect/strip injection patterns from context
- Output validation: schema/type checks on tool parameters, idempotency keys, side-effect “dry-run” or simulate before commit
- Observability & audit: step-level tracing with redaction, log policy decisions (allow/deny/escalate), agent/role versions, and tool scopes
- Testing & drills: red-team prompts, injection test suites, and replay of known attacks in CI, staged deploys with canary/shadow and automatic rollback on safety breaches

*Further reading:* [Databricks AI Security Framework (DASF): a structured approach to securing AI systems](https://www.databricks.com/blog/introducing-databricks-ai-security-framework-dasf)