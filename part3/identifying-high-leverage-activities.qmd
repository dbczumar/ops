# Prioritizing high-leverage activities

## Introduction: What to focus on first? 

Our customers are never short on ideas for how generative AI can improve their company's processes. Indeed, some customer's backlog run up to hundreds of use cases. But they do frequently ask, "what should I focus on first when developing my agent?"

This question is not surprising. Agentic workflows are an order of magnitude more complex than the linear RAG chains that were the main focus of 2024. While a basic RAG pipelines may include a predictable set of steps such as retrieval and generation, agentic workflows more open-ended and less well-scoped. They usually involve open-ended reasoning, dynamic tool selection, and automation of intricate multi-step business workflows.

Take insurance claims processing as an example. An insurance claim can go through multiple rounds of asking for evidence for a claim, verifying that evidence, and asking the customer for more information. 

In another example, a telco customer support agent that investigates an unexpectedly high bill charge may need to first identify the cause of the anomaly, and then wait for a human to approve a refund. 

To address this complexity, we've identified a repeatable sequence of planning activities that we've found to be important for mapping out our problem space and for aligning stakeholders. Focus on these key areas at the start of a project, and the project's chances of success increase greatly. 

These activities can be implemented as part of *design sprint*. Design sprints are time-boxed collaborative workshops that bring together cross-functional teams to rapidly prototype and validate solutions before committing significant development resources. Originally popularized by Google Ventures, design sprints compress months of traditional planning into an intensive, focused period where teams can align on design decisions, identify key data sources and plan evaluation suites before diving into the complex technical implementation.

## Key planning activities to prioritize

To address this complexity, we've identified a repeatable sequence of planning activities that we've found to be important for mapping out our problem space and for aligning stakeholders. Focus on these key areas at the start of a project, and the project's chances of success increase greatly. 

These activities can be implemented as part of a roughly two-week design and planning workshop at the start of a project, something like a design sprint. Design sprints are time-boxed collaborative workshops that bring together cross-functional teams to rapidly prototype and validate solutions before committing significant development resources. Originally popularized by Google Ventures, design sprints compress months of traditional planning into an intensive, focused period where teams can align on design decisions, identify key data sources and plan evaluation suites before diving into the complex technical implementation.

### Suggested planning sequence

1. Map out in detail the current workflow as it is executed by humans
2. Translate the business workflow mapped in step 1 into a technical architecture covering:
    - the required structured and unstructured data sources
    - which aspects of the workflow should be handled by tool-calling sub-agents or by linear workflows 
    - how each sub-component should be evaluated for quality  
3. List out the information that different stakeholders, from C-level executives to business users to developers need from the agentic application
4. Design tracing and logging modules to capture the information identified in step 3. 
5. Map the access controls each agent and final end-user requires.  
6. Future planning: identify components that other projects can reuse and build abstractions accordingly

To make these steps more concrete, we will go through a worked example of a customer support agent designed to handle customer queries sent to a telecommunications company

## Worked example: Telecommunications Customer Support Agent

### 1. Business process mapping

In our example, support agents primarily respond to inbound customer calls, emails, live chats, and support tickets regarding service disruptions, billing questions, plan changes, and technical problems . They must quickly diagnose issues ranging from simple password resets to complex network connectivity problems. The diagram below shows the broad categories of questions an agent might receive as well as some example questions. Note also that some questions might span multiple categories, and multiple categories might also be addressed within a single customer call. 

![high level workflow breakdown](./files/high-level-workflow-breakdown.png)

Addressing a customer query is not always straightforward. As shown in the workflow below, when handling a question related to an unexpectedly high bill, an agent may have to iteratively work through a set of scenarios to identify a root cause. This workflow involves a combination of planning, taking action and evaluating the action's output before deciding on a next step, making it suitable to be handled by an agentic solution rather than a linear LLM workflow. 

![billing-specific workflow breakdown](./files/billing-query-workflow.png)

### 2. Translating business process into a technical architecture

From mapping out how human customer support agents manage their responsibilities, we understand that they commonly need to address queries around four broad areas: account, billing, products and technical support. 

Although a single human can easily mentally classify and handle customer queries across all these areas, agents are different. We want to ensure what each agent has a defined set of responsibilities and toolset. By designing for modularity, we can better construct prompts, tools and evaluations. 

Therefore, we decide on having one sub-agent per query category, with a supervisor agent being responsible for routing questions between agents. We do not expect sub-agents to communicate with each other, which simplifies our setup and reduces protential sources of indeterminism. 

**Multi-agent high-level architecture**

![Multi-agent example architecture](./files/telco-multi-agent-example.png)

| Agent | Primary Role | Description |
|-------|-------------|-------------|
| **Supervisor Agent** | Workflow Orchestration | Orchestrates the workflow, routes queries to specialized agents, generates responses. Conducts query intent classification, sentiment analysis, and extracts attributes for routing decisions |
| **Account Agent** | Customer Profile Management | Handles customer profile and subscription queries |
| **Billing Agent** | Financial Services | Processes billing, payment, and usage-related queries |
| **Tech Support Agent** | Technical Assistance | Provides troubleshooting and technical support |
| **Product Agent** | Product Information | Manages queries about plans, devices, and promotions |

Additionally, we should also map out the tools we can provide to the agent

**Billing sub-agent setup**

![Billing agent tools](./files/billing-agent-tools.png)

### 3. Identify available data sources 

Structured data sources

| Data Source | Schema Summary |
|-------------|----------------|
| telco_customer_support_dev.bronze.customers | Customer profile data including customer_id, segment, location, registration_date, status, contact preferences, and scoring metrics (loyalty_tier, churn_risk_score, customer_value_score, satisfaction_score) |
| telco_customer_support_dev.bronze.subscriptions | Subscription records linking customers to plans/devices with subscription_id, customer_id, plan_id, device_id, promo_id, dates, contract terms, charges, and status |
| telco_customer_support_dev.bronze.plans | Service plan catalog containing plan_id, name, type, pricing, data limits, feature flags (unlimited calls/texts), contract requirements, and descriptions |
| telco_customer_support_dev.bronze.devices | Device catalog with device_id, name, manufacturer, type, pricing (retail/installment), specifications (storage, 5G compatibility), colors, and availability status |
| telco_customer_support_dev.bronze.promotions | Promotional offers including promo_id, name, discount details (type/value), date ranges, descriptions, and active status |
| telco_customer_support_dev.bronze.billing | Billing records with billing_id, customer/subscription references, dates, charge breakdowns (base, additional, tax), payment information, and status |
| telco_customer_support_dev.bronze.usage | Usage metrics containing usage_id, subscription_id, daily usage data (data_usage_mb, voice_minutes, sms_count), and billing cycle information |


Unstructured data sources 

| Table | Unstructured Content |
----------|-------------------------------|
| telco_customer_support_dev.bronze.knowledge_base | Full content stored as markdown text including FAQs, policies, guides, and procedures |
| telco_customer_support_dev.bronze.support_tickets | Free-text issue descriptions from customers |
| telco_customer_support_dev.bronze.support_tickets | Free-text resolution details from support agents |


### Instrumentating observability based on personas

TODO: something here about involving stakeholders early in the planning process and providing good information with user interfaces for internal stakeholders? 

In the chapter Principle 3: The technical practices of learning, we talked at length about how to use tracing to provide visibility into the workings of our agentic application. In this section, we want to expand on this section to cover how traces might be used to provide different stakeholders with a view on how the agent is functioning, in a way that addresses what they care about. 

Building for different personas: 

1. C-suite executives: cares about optics - 

- risk and compliance
- audit trail completeness 
- overall claims distribution

business metrics

- processing cost per claim 
- ROI versus traditional processing methods 
- settlement accuracy

quality and accuracy indicators
- agent decision accuracy rate compared to human expert reviews
- escalation rate to human agents
- quality score trends over time 
- false positive / negative rates for claim approvals

2. Business users: cares about interpretability, understand how agents are executing different steps in the workflow so they can troubleshoot / provide inputs if needed. Still want autonomy over the workflows as a human-in-the-loop
3. Developers: want as much granularity as possible. detailed telemetry not only into inputs and outputs, but operational metrics and latency bottlenecks as well. 

![end-user-view](./files/data-int-off.png)

![business SME view](./files/data-int-on.png)

![developer view](./files/op-metrics.png)

### Designing test suites 
#### Using top-down and bottom-up approaches together

As we design our multi-agent workflow, certain aspects of the design will make it clear what evaluations we may need. For example, a billing agent would need a tool to query a table for customer spend therefore, we should create a test that ensures that this tool works correctly. This is an evaluation that can be identified upfront without analyzing any traces. 

At the same time, certain failure modes only become evident after a bottoms-up analysis of LLM inputs or outputs. For example, an LLM step to generate SQL that queries a database table may mix up different product acronyms. For these failure modes, we have to conduct an analysis 

Here, we outline some potential ways to get started with both top-down and bottom-up evaluation suite design in order to setup a project for success. 

**Top-down approach** 

Generate various customer personas and scenarios, have an LLM create sample queries 

```
    "billing": {
        "contexts": [
            QueryContext("billing", True, True, "concerned customer", "bill inquiry"),
            QueryContext("billing", True, True, "budget-conscious customer", "payment planning"),
            QueryContext("billing", True, False, "confused customer", "charge explanation"),
            QueryContext("billing", True, True, "data-heavy user", "usage verification"),  # Enhanced
            QueryContext("billing", True, True, "traveling customer", "roaming charges"),
            QueryContext("billing", True, True, "usage-monitoring customer", "usage tracking"),
            QueryContext("billing", True, True, "business customer", "usage reporting"),
            QueryContext("billing", True, True, "family plan customer", "usage analysis"),
        ],
        "base_scenarios": [
            "customer sees unexpected charges on their bill",
            "customer wants to know when payment is due",
            "customer needs breakdown of current month charges",
            "customer is questioning data usage amounts", 
            "customer wants payment history for tax purposes",
            "customer needs to understand prorated charges",
            "customer is asking about auto-pay status",
            "customer wants to dispute a specific charge",
            "customer needs usage details for expense reporting",
            "customer is planning data usage for upcoming month",
            "customer wants to know their data usage for a specific month",
            "customer needs to check voice minutes used in last billing cycle", 
            "customer is asking about SMS usage over a date range",
            "customer wants to compare usage between different months",
            "customer needs total usage breakdown for expense reporting",
            "customer is checking if they're approaching data limits",
            "customer wants to analyze usage patterns over time",
            "customer needs usage details for a specific billing period",
            "customer is tracking usage to optimize their plan",
            "customer wants to know peak usage periods",
            "customer needs usage data for tax deduction purposes",
            "customer is monitoring family member usage on shared plan",
        ]
    },
```    
**Bottoms-up approach**

Conduct error analysis based on sample traces, then align LLM judge outputs with SME feedback to create automated tests

This exercise is best down in a workshop format with SMEs, developers and other stakeholders This is

TODO: Align with Pallavi and Jonathan to include this section here based on workshops they did with customers

Most judge builders on the market need to assume that teams a) know what to judge and b) how to collect quality data. While this is true for many customers, there are also many for whom it is not. 

This workshop additionally provides:
- Guided error analysis to identify what judges are high value
- Proven methodology for collecting high-quality data
- Tools for annotation, IRR computation, and disagreement resolution
- Compatibility with MLFlow traces
- This gives teams Surge/Scale/SuperAnnotate-style capabilities in-house, with their SMEs, for a fraction of the time and cost. They can then take these gold labels to our Judge Builder Product for auto-optimization.

##### Workshop outline 

##### Workshop outcomes

### Future planning: identify components that other projects can reuse and build abstractions accordingly

Base classes for agents

- has uniform way of initializing configurations
- configuration variables standardised eg. vector search endpoints, sub config object for managing UC assets

Base classes for evaluation scorers 

- do the same thing with evaluation scorers 

Can also consider doing the same for various tracing EDA tools eg. summary report that can be used as a template and customised by different use case teams 

TODO: include example from custom mlflow review repository

```
class BaseAgent(ResponsesAgent, abc.ABC):
    """Base agent class all agents inherit from."""

    _config_cache: dict[str, AgentConfig] = {}

    def __init__(
        self,
        agent_type: str,
        llm_endpoint: Optional[str] = None,
        tools: Optional[list[dict]] = None,  # UC function tools
        vector_search_tools: Optional[
            dict[str, Any]
        ] = None,  # Map of tool name -> VectorSearchRetrieverTool (not UC function)
        system_prompt: Optional[str] = None,
        config_dir: Optional[Path | str] = None,
        inject_tool_args: Optional[list[str]] = None,
        disable_tools: Optional[list[str]] = None,
        uc_config: Optional[UCConfig] = None,
    ):
        """Initialize base agent from config file.

        Args:
            agent_type: Type of agent (used for config loading)
            llm_endpoint: Optional override for LLM endpoint
            tools: Optional list of UC function tools
            vector_search_tools: Optional dict mapping tool names to VectorSearchRetrieverTool objects
            system_prompt: Optional override for system prompt
            config_dir: Optional directory for config files
            inject_tool_args: Optional list of additional tool arguments to be injected into tool calls from custom_inputs.
            disable_tools: Optional list of tool names to disable. Can be simple names or full UC function names.
            uc_config: Optional UC configuration for Unity Catalog resources
        """
```

Base classes for evaluation scorers 

```
from abc import ABC, abstractmethod
from typing import Any, Optional

from mlflow.entities import Feedback, Trace
from mlflow.genai.judges import custom_prompt_judge, meets_guidelines
from mlflow.genai.scorers import scorer
from mlflow.genai.scorers.builtin_scorers import BuiltInScorer


class BaseScorer(ABC):
    def __init__(self, name, sample_rate):
        self.name = name
        self.sample_rate = sample_rate

    @abstractmethod
    def get_offline_scorer(self):
        """Implementation of custom scorer for online evaluation."""
        pass

    @abstractmethod
    def get_online_scorer(self):
        """Implementation of custom metric for offline evaluation."""
        pass
```

Data classes